{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partycjonowanie i glooming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Kiedy RDD jest tworzone, możesz określić liczbę partycji\n",
    "* Defaultowo jest liczbą executorów definiowaną podczas ustawienia SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import findspark\n",
    "from settings import SPARK_PATH\n",
    "findspark.init(SPARK_PATH) # Ścieżka do Sparka "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "A=sc.parallelize(range(1000000))\n",
    "print(A.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repartycjonowanie "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "D = A.repartition(10)\n",
    "print(D.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Można również zdefiniować liczbę partycji podczas tworzenia RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "A = sc.parallelize(range(1000000),numSlices=10)\n",
    "print(A.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partycjonowanie w celu równoważenia obciążenia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Załóżmy, że zaczynamy od 10 partycji, wszystkie z dokładnie taką samą liczbą elementów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100000, 100000, 100000, 100000, 100000, 100000, 100000, 100000, 100000, 100000]\n"
     ]
    }
   ],
   "source": [
    "A=sc.parallelize(range(1000000))\\\n",
    "    .map(lambda x:(x,x)).partitionBy(10)\n",
    "print(A.glom().map(len).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Załóżmy że używamy funkcji filter w celu selekcji pewnych elementów w zbiorze A\n",
    "* Pewnie partycje mogą mieć więcej elementów niż inne "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100000, 0, 0, 0, 0, 100000, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "#select 10% of the entries\n",
    "B=A.filter(lambda pair: pair[0]%5==0)\n",
    "# get no. of partitions\n",
    "print(B.glom().map(len).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Przyszłe operacje na zbiorze B będą używane tylko na dwóch workerach\n",
    "* **Inne workery nie będą robiły nic** ze względu na to że ich partycje będą puste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jak można temu zapobiec?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* W celu efektywnego przetwarzania należy zrepartycjonować RDD \n",
    "* Jednym ze sposobów jest repartycjonowanie z wykorzystaniem klucza "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Metoda .partitionBy(k) przyjmuje (klucz,wartość) RDD gdzie klucze są integer.\n",
    "* Partycjonowanie RDD na k partycji\n",
    "* Element (key,value) jest umieszczany w key % k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20000, 20000, 20000, 20000, 20000, 20000, 20000, 20000, 20000, 20000]\n"
     ]
    }
   ],
   "source": [
    "C=B.map(lambda pair:(pair[1]/10,pair[1])).partitionBy(10) \n",
    "print(C.glom().map(len).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inny sposób  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Innym sposobem jest użycie automatycznego partycjonowania wykorzystując repartition(k)\\\n",
    "\n",
    "* Zaletą rozwiązania jest brak konieczności definiowania klucza\n",
    "* Wadą jest brak kontroli na partycjonowaniem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20000, 20000, 20000, 20000, 20000, 20000, 20000, 20000, 20000, 20000]\n"
     ]
    }
   ],
   "source": [
    "C=B.repartition(10)\n",
    "print(C.glom().map(len).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glom()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **glom()** transformuje każdą partycję na tuple(immutable list) \n",
    "* Tworzy RDD zawierające tuple\n",
    "* workery mogą odnosić się do elementów po indeksie partycji\n",
    "* dalej nie możesz assignować wartości do poszczególnych elementów, RDD dalej jest immutable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20000, 20000, 20000, 20000, 20000, 20000, 20000, 20000, 20000, 20000]\n"
     ]
    }
   ],
   "source": [
    "print(C.glom().map(len).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "python",
   "name": "spark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
